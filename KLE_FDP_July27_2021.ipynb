{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <span style=\"color:blue\">An overview on ML/DL model development</span>\n",
    "<span style=\"color:blue\">AICTE Training and Learning (ATAL) Academy Sponsored FDP on **DL for Audio and Speech Processing**\n",
    "</span>\n",
    "\n",
    "<div style=\"text-align: right\">Dr. Sishir Kalita</div>\n",
    "<div style=\"text-align: right\">Data Scientist</div>\n",
    "<div style=\"text-align: right\">Armsoftech.air</div>\n",
    "<div style=\"text-align: right\">Chennai</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "2. Building a DL model: steps\n",
    "\n",
    "3. Bias/variance problem\n",
    "\n",
    "4. Regularizations and batch norm\n",
    "\n",
    "5. ML Strategies\n",
    "\n",
    "6. Evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Different Speech Tech projects\n",
    "\n",
    "1. Voice bot (Speech enhancement, ASR, TTS, Speech analytics, SLU)\n",
    "2. Speaker recognition\n",
    "3. Spoken Language Idenfication\n",
    "4. Speech to speech translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key steps in building an ML/DL model\n",
    "\n",
    "1. Problem definition\n",
    "2. Gathering data\n",
    "3. Data pre-processing\n",
    "4. Define the evaluation metric\n",
    "5. Model training / testing \n",
    "    * Iterate till you get the good results in the test set\n",
    "    * Pump more labeled data\n",
    "6. Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"idea.png\" width=\"700\" /> \n",
    "\n",
    "Image source: <a href=\"https://www.coursera.org/learn/machine-learning-projects\" target=\"_blank\">Coursera | Deep Learning Specialization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Where will I get the speech data?\n",
    "\n",
    "* Your own customized data\n",
    "* <a href=\"https://datasetsearch.research.google.com/\" target=\"_blank\">Google dataset search</a> \n",
    "* <a href=\"https://www.kaggle.com/datasets\" target=\"_blank\">Kaggle</a> \n",
    "* <a href=\"https://commonvoice.mozilla.org/en/datasets\" target=\"_blank\">Commonvoice Mozilla</a> \n",
    "* <a href=\"https://openslr.org/resources.php\" target=\"_blank\">Openslr</a> \n",
    "* <a href=\"https://nplt.in/demo/resources/speech-corpus\" target=\"_blank\">National Platform for Language Technology</a> \n",
    "* <a href=\"https://catalog.ldc.upenn.ed\" target=\"_blank\">Linguistic Data Consortium</a> \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prepare your train, development (dev) / validation (val) and test (evaluation (eval)) sets\n",
    "1. How to divide:\n",
    "    * Before deep learning era: <span style=\"color:blue\">80% (train: 8000), 10% (Dev: 1000), and 10% (Test: 1000)</span>  (**if you have 10,000 examples**)\n",
    "    * Present days : <span style=\"color:blue\">98% (train), 1% (Dev), and 1% (Test)</span> (**if you have 10,00,0000 examples***)\n",
    "1. Should have a more diverse <span style=\"color:blue\">train</span> set\n",
    "2. Distribution of <span style=\"color:blue\">dev</span> and  <span style=\"color:blue\">test</span> sets should be same\n",
    "3. Cleaning up mislabeled <span style=\"color:blue\">dev</span> and  <span style=\"color:blue\">test</span> sets examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Steps to train a model\n",
    "\n",
    "1. Decide the model and define its structure\n",
    "2. Normalizing the inputs\n",
    "3. Initialize the parameters of the model\n",
    "4. Learn the parameters for the model by minimizing the cost\n",
    "    * Calculate current loss (forward propagation)\n",
    "    * Calculate current gradient (backward propagation)\n",
    "    * Update parameters (gradient descent)\n",
    "5. Use the learned parameters to make predictions (on the test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Let's consider a logistic regression model**\n",
    "* Binary classification model\n",
    "* Training set: $ \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ... (x^{(m)}, y^{(m)})\\} $\n",
    "\n",
    "\\begin{align}\n",
    "\\hat y = h_{\\text{w}}(x) = \\frac{1}{1 + e^{-\\text{w}^Tx}}\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "    {\\text{w}}^Tx &= [\\text{w}_{1},\\text{w}_{2},\\text{w}_{3}]\n",
    "          \\begin{bmatrix}\n",
    "           x_1 \\\\\n",
    "           x_2 \\\\\n",
    "           x_{3}\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}\n",
    "  \n",
    "<p float=\"right\">\n",
    "<img src=\"sigmoid.png\" width=\"300\" align=\"left\"/>\n",
    "<img src=\"sigmoid1.png\" width=\"400\" align=\"left\"/> \n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Compute the loss function: ('m' is the number of training example)**\n",
    "- **Binary cross-entropy loss**\n",
    "\\begin{align}\n",
    "J(\\text{w}, b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat y^{(i)}, y^{(i)})\n",
    "= -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(\\hat y^{(i)}) + (1 - y^{(i)})log(1- \\hat y^{(i)})]\n",
    "\\end{align}\n",
    "If y = 1 (first part) and y = 0 (second part)\n",
    "\n",
    "Now, we need to find the **w** and **b** which minimize the $J(\\text{w}, b)$ [Requires one optimization algorithm]\n",
    "\n",
    "<img src=\"costng.png\" width=\"500\" /> \n",
    "\n",
    "Image source: <a href=\"https://cs230.stanford.edu/files/C1M2.pdf\" target=\"_blank\">cs230.stanford.edu</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization algorithms (I hope it is already discussed)\n",
    "\n",
    "**Gradient descent**\n",
    "1. Compute the gradient w.r.t **w** and **b**\n",
    "2. Update the **w** and **b**\n",
    "\\begin{align}\n",
    "w_{j+1} = w_{j} - \\alpha \\frac{dJ(w,b)}{dw}\\\\\n",
    "b_{j+1} = b_{j} - \\alpha \\frac{dJ(w,b)}{db}\n",
    "\\end{align}\n",
    "3. Iterate till you reach local minima\n",
    "\n",
    "<p float=\"right\">\n",
    "<img src=\"loss.png\" width=\"300\" align=\"left\"/> \n",
    "<img src=\"sgd.gif\" width=\"400\" align=\"left\"/> \n",
    "</p>\n",
    "\n",
    "Image source: <a href=\"http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/\" target=\"_blank\">rasbt.github.io</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other loss functions\n",
    "- **Focal loss:** It reshapes the cross entropy loss in such a way that it down weighs the loss assigned to well classified examples\n",
    "- **Negative log likelihood loss (NLLL):** takes class weights as input\n",
    "- **Constrastive loss**\n",
    "- **Connectionist Temporal Classification Loss (CTC Loss):** where we need alignment between sequences\n",
    "\n",
    "\n",
    "[Focal Loss](https://medium.com/adventures-with-deep-learning/focal-loss-demystified-c529277052de)\n",
    "[CTC Loss](https://distill.pub/2017/ctc/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Stochastic gradient descent** vs **mini batch gradient descent** vs **Batch gradient descent**\n",
    "1. Let's say, your train set size 'm'\n",
    "2. Stochastic gradient descent: calculate error for each example and update the model for each example\n",
    "3. Mini batch gradient descent: take a mini batch (M < m), compute the error, and update the model for each mini-batch\n",
    "4. Batch gradient descent: calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated\n",
    "\n",
    "**Other advanced optimization algorithms**\n",
    "1. Gradient descent with momentum\n",
    "2. Adam\n",
    "3. RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is epoch?\n",
    "1. One cycle through the entire training dataset is called a training epoch\n",
    "2. Number of passes (1 pass : one forward pass + one backward pass in one batch)\n",
    "3. Let m = 1000, and M = 10\n",
    "4. For SGD: there will be 1000 iterations/epoch\n",
    "5. For Mini batch gradient descent: there will be 1000/10 (100) iterations/epoch\n",
    "6. For Batch gradient descent: there will be 1 iteration/epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        ##################\n",
    "        # Forward pass\n",
    "        # Makes predictions\n",
    "        yhat = model(x_batch)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y_batch, yhat)\n",
    "        \n",
    "        ##################\n",
    "        # Backward pass\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        ##################\n",
    "        # Updates parameters\n",
    "        optimizer.step()\n",
    "                \n",
    "    with torch.no_grad():\n",
    "        for x_eval, y_eval in eval_loader:\n",
    "            x_eval = x_eval.to(device)\n",
    "            y_eval = y_eval.to(device)\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_eval)\n",
    "            eval_loss = loss_fn(y_eval, yhat)\n",
    "            eval_losses.append(eval_loss.item())\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalization of the inputs\n",
    "<p float=\"right\">\n",
    "<img src=\"norminput1.png\" width=\"400\" align=\"left\"/>\n",
    "<img src=\"norminput2.png\" width=\"250\" align=\"left\"/> \n",
    "</p>\n",
    "\n",
    "Image source: <a href=\"deeplearning.ai\" target=\"_blank\">Andrew Ng</a>\n",
    "\n",
    "For train set:\n",
    "\\begin{align}\n",
    "X^{train}_{norm} = \\frac{X^{train} - \\mu^{train}}{\\sigma^{train}}\n",
    "\\end{align}\n",
    "\n",
    "For test set:\n",
    "\\begin{align}\n",
    "X^{test}_{norm} = \\frac{X^{test} - \\mu^{train}}{\\sigma^{train}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why we should normalize the input\n",
    "\n",
    "<p float=\"right\">\n",
    "<img src=\"norminput3.png\" width=\"1000\" align=\"left\"/> \n",
    "</p>\n",
    "\n",
    "Image source: <a href=\"deeplearning.ai\" target=\"_blank\">Andrew Ng</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias/Variance problem\n",
    "\n",
    "1. We can get some idea for improving the model by knowing the bias/variance problem\n",
    "\n",
    "|  | Train set error (%) | Test set error (%) | Conclusion | \n",
    "| :-: | :-: | :-: | :-: | \n",
    "| High bias problem | 25 | 40 | Underfitting | \n",
    "| High variance problem | 1 | 12 | Overfitting | \n",
    "| Good model | 1 | 2| \n",
    "\n",
    "<img src=\"bias_var.png\" width=\"800\">\n",
    "Image source: <a href=\"deeplearning.ai\" target=\"_blank\">deeplearning.ai</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Addressing bias/variance problem\n",
    "1. **Reducing the high variance**\n",
    "\n",
    "    * Add more training data\n",
    "    * Add regularization \n",
    "        * L2 regularization\n",
    "        * Dropout\n",
    "    * Add early stopping\n",
    "    * Decrease the model size\n",
    "\n",
    "2. **Reducing the high bias**\n",
    "\n",
    "    * Increase the model size (# of neurons/layers)\n",
    "    * Try to run it longer\n",
    "    * Different (advanced) optimization algorithms\n",
    "    * Reduce or eliminate regularization\n",
    "    * Modify model architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**<span style=\"color:blue\">Try until you get better results on both train and test sets</span>**\n",
    "\n",
    "<img src=\"bias-variance-tradeoff.png\" width=\"300\">\n",
    "\n",
    "Iamge source: <a href=\"https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/\" target=\"_blank\">dziganto.github.io</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### L2 Regularization\n",
    "\\begin{align}\n",
    "J(\\text{w}) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat y^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\sum_{i=1}^{m}(|\\text{w}^{(i)}|^2)\n",
    "\\end{align}\n",
    "1. Here, $\\lambda$ is the regularization parameter (hyperparameter)\n",
    "2. Penalizes large weights and effectively limits the freedom the model\n",
    "3. Causes the weight to decay in proportion to its size\n",
    "3. If lambda is too large - a lot of **w**'s will be close to zeros which will make the NN simpler (you can think of it as it would behave closer to logistic regression).\n",
    "\n",
    "<img src=\"reg_strengths.jpeg\" width=\"400\">\n",
    "\n",
    "Image source: <a href=\"https://cs231n.github.io/neural-networks-1/\" target=\"_blank\">cs231n.github.io</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dropout\n",
    "\n",
    "<img src=\"drop1.png\" width=\"600\">\n",
    "Image source: <a href=\"https://cs231n.github.io/neural-networks-2/\" target=\"_blank\">cs231n.github.io</a>\n",
    "\n",
    "1. The dropout regularization eliminates some neurons/weights on each iteration based on a probability\n",
    "2. Can’t rely on any one feature, so have to spread out weights [Andrew Ng]\n",
    "\n",
    "Demo: <a href=\"https://qmsvpvzwwppdeofafmdkgv.coursera-apps.org/notebooks/week5/Regularization/Regularization_v2a.ipynb\" target=\"_blank\">L2 & Dropout</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other regularization methods\n",
    "1. Data augmentation\n",
    "    * If data mismatch between train and test set\n",
    "    * Add noise in the speech signal or perturb the speech signal\n",
    "    * Use speech synthesis\n",
    "    * Distorts the image (scaled, rotate)\n",
    "    * Create image using graphics\n",
    "2. Early stopping\n",
    "    * Check the train and validation set errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters and Hyperparameters\n",
    "1. Weights (w) or bias (b) is a learnable parameter\n",
    "2. Hyper parameters (parameters that control the algorithm)\n",
    "    * Learning rate\n",
    "    * Number of iteration\n",
    "    * Number of hidden layers\n",
    "    * Number of hidden units\n",
    "    * Choice of activation functions\n",
    "    * Mini-batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalizing activations in a network\n",
    "**Batch normalization (BN)**\n",
    "1. BN allows each layer of a network to learn by itself a little bit more independently of other layers\n",
    "2. Reduces the problem of input values changing (shifting)\n",
    "\n",
    "<p float=\"right\">\n",
    "<img src=\"batch-normalization.jpg\" width=\"400\" align=\"left\">\n",
    "<img src=\"val.png\" width=\"300\" align=\"left\">\n",
    "</p>\n",
    "\n",
    "Image source: <a href=\"https://www.learnopencv.com/batch-normalization-in-deep-networks/\" target=\"_blank\">earnopencv</a>\n",
    "\n",
    "\n",
    "```python\n",
    "model = Sequential\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some strategies while developing an  ML/DL model-\n",
    "1. Carrying out error analysis\n",
    "2. Pretraining\n",
    "    * Transfer learning\n",
    "    * Self-supervised learning\n",
    "3. Multi-task learning\n",
    "4. End-to-end modeling\n",
    "5. Domain adaption\n",
    "6. Self-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Carrying out error analysis\n",
    "\n",
    "  * Error analysis - process of manually examining mistakes that your algorithm is making.\n",
    "  * It can give you insights into what to do next\n",
    "  * Cleaning up incorrectly labeled data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pretraining - Transfer Learning (TL) and Self-supervised Learning (SSL)\n",
    "\n",
    "- Pretraining has become a standard technique in CV, NLP\n",
    "- Transfer learning (TL) uses labeled data to learn a good representation network - Supervised fashion\n",
    "- Self-supervised learning does not require annotated labels\n",
    "\n",
    "<img src=\"pretrain.png\" width=\"600\">\n",
    "\n",
    "Image source: <a href=\"https://arxiv.org/pdf/2007.04234.pdf\" target=\"_blank\"> A Tale of Two Pretraining Paradigms</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transfer learning\n",
    "\n",
    "- Let's consider we have on ASR model for Tamil.\n",
    "- Can we use that model to train an ASR model for Telugu?\n",
    "\n",
    "<p float=\"right\">\n",
    "<img src=\"cnn1.png\" width=\"400\" align=\"left\">\n",
    "</p>\n",
    "\n",
    "Image source: <a href=\"https://arxiv.org/pdf/2007.04234.pdf\" target=\"_blank\">towardsdatascience</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-supervised based pretraining model\n",
    "- What is self-supervised learning?\n",
    "  * Self-supervised learning obtains supervisory signals from the data itself \n",
    "  * Labels are naturally part of the input data\n",
    "  * Learn general data representations from unlabeled examples\n",
    "  * Fine tuning for your downstream tasks\n",
    "  \n",
    "<p float=\"right\">\n",
    "<img src=\"ssl.png\" width=\"500\" align=\"center\">\n",
    "</p>  \n",
    "  \n",
    "[Self-supervised-learning](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wav2vec2\n",
    "\n",
    "* Wav2Vec2 learns powerful speech representations from large amount of unlabeled speech\n",
    "* It learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network\n",
    "* **Pretraining** and **Finetuning**\n",
    "* [Facebook Pretrained model](https://github.com/pytorch/fairseq/blob/master/examples/wav2vec/README.md)\n",
    "  \n",
    "<p float=\"right\">\n",
    "<img src=\"wav2vec21.png\" width=\"500\" align=\"center\">\n",
    "</p>  \n",
    "  \n",
    "[wav2vec2](https://arxiv.org/abs/2006.11477)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wav2vec2 for ASR\n",
    "\n",
    "* Wav2Vec2 is fine-tuned using CTC loss with transcribed data\n",
    "\n",
    "\n",
    "<p float=\"right\">\n",
    "<img src=\"w2v_results.png\" width=\"500\" align=\"center\">\n",
    "</p> \n",
    "\n",
    "[wav2vec2](https://arxiv.org/abs/2006.11477)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wav2vec2 learned speech embeddings for other downstrem task\n",
    "\n",
    "- **[Speaker verification and language identification](https://arxiv.org/pdf/2012.06185.pdf)**\n",
    "- **[Emotion recognition](https://arxiv.org/abs/2104.03502)**\n",
    "- **[ASR model development for low-resource language](https://arxiv.org/abs/2012.12121)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ASR Development uisng wav2vec2 for Indian language\n",
    "\n",
    "1. Fine-tuned on three different databases provided by IITM\n",
    "  - **Pretained model: [XSLR-53](https://github.com/pytorch/fairseq/blob/master/examples/wav2vec/README.md) | Trained on 56000 hours of speech data of 53 different languages**\n",
    "\n",
    "| Language | Train (Hours)  | Eval (Hours)  | WER | LM (kenLM)\n",
    "| :-: | :-: | :-: | :-: |  :-: |\n",
    "| Indian English |  179.5 | 5.4 | 4.91 % | 6 Gram |\n",
    "| Hindi | 178.4 |  4.9 | 4.55 %| 5 Gram |\n",
    "| Tamil | 104.5  | 3.8 | 5.84 %| 4 Gram|\n",
    "\n",
    "2. Kaldi based TDNN model\n",
    "\n",
    "| Language | Train (Hours)  | Eval (Hours)  | WER  | LM (SRILM)\n",
    "| :-: | :-: | :-: | :-: | :-: | \n",
    "|  Indian English |  179.5 | 5.4 | 4.97 % | RNNLM Rescore |\n",
    "| Hindi | 178.4 |  4.9 | 3.73 %| 5 Gram |\n",
    "| Tamil | 104.5  | 3.8 | 5.21 %| 5 Gram |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wav2vec2 pretrained model Indic languages\n",
    "\n",
    "- [EkStep Models](https://github.com/Open-Speech-EkStep/vakyansh-models)\n",
    "- [Paper](https://arxiv.org/pdf/2107.07402.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-task learning\n",
    "1. One neural network learns several tasks at the same time\n",
    "2. Each of these tasks helps all of the other tasks\n",
    "\n",
    "<p float=\"right\">\n",
    "<img src=\"mul.png\" width=\"500\" align=\"left\">\n",
    "</p>\n",
    "\n",
    "Image source: <a href=\"https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2594.pdf\" target=\"_blank\">isca</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### End-to-end modelling\n",
    "* No feature engineering and no intermediate stages\n",
    "* Need large amount of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation metric:\n",
    "\n",
    "**Highly depends on your application and what you are trying to optimize for**\n",
    "\n",
    "1. Confusion matrix\n",
    "2. Accuracy\n",
    "3. Precision / Recall / F1 score\n",
    "4. Word error rate in ASR\n",
    "5. Bilingual Evaluation Understudy (BLEU) Score in machine translation\n",
    "6. Equal error rate in speaker verifiction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Basic metrics you should know**\n",
    "\n",
    "| H/P | + class | - class | \n",
    "| :-:| :-: | :-: | \n",
    "| **+ class** | True positive | False negative | \n",
    "| **- class** | False positive | True negative | \n",
    "\n",
    "**[When FP and FN will be useful!]**\n",
    "1. FN should be zero: cancer diagnosis\n",
    "2. FP should be zero: speaker verification\n",
    "\n",
    "Accuracy = $\\frac{TP + TN}{TP + FP + FN + TN}$\n",
    "\n",
    "Precision (proportion of positive identifications which was actually correct) = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "Recall = (proportion of actual positives which was identified correctly) = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "F1 score = HM(Precision, Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "References:\n",
    "1. Structuring Machine Learning Projects : <a href=\"https://www.coursera.org/learn/machine-learning-projects\" target=\"_blank\">Andrew Ng</a> \n",
    "2. Visualization of ML techniques : <a href=\"https://gfycat.com/gifs/search/gradient+descent\" target=\"_blank\">egfycat</a>\n",
    "3. CNN materials: <a href=\"http://cs231n.stanford.edu/\" target=\"_blank\">cs231n.stanford.edu</a> \n",
    "4. Andrew Ng DL notes: <a href=\"https://cs230.stanford.edu\" target=\"_blank\">cs230.stanford.edu</a>\n",
    "5. MOOCS: Coursera & EDx \n",
    "6. Read ML articles in https://medium.com\n",
    "7. <a href=\"https://d2wvfoqc9gyqzf.cloudfront.net/content/uploads/2018/09/Ng-MLY01-13.pdf\" target=\"_blank\">Machine Learning Yearning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### You can reach me @\n",
    "\n",
    "1. email: sisiitg@gmail.com\n",
    "2. mobile no.: 9435379331 \n",
    "3. [Linkedin](https://www.linkedin.com/in/sishir-kalita-3a006120/)\n",
    "\n",
    "\n",
    "### THANK YOU!\n",
    "### Stay safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
